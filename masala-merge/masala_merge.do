qui {

  /**********************************************************************************/
  /* Meta-algorithm:
  1. Clean each dataset, drop groups that only occur in one of the datasets.
  2. Identify the manual matches (if any) and remove them from the datasets.
  3. Run lev_merge and reclink, drop unclear matches.
  4. Combine the matches found in lev_merge, reclink, and manual matches.
  5. Merge in the unmatched data from both the master and using datasets.
  6. Output the non-matches in a csv to allow the user to create new manual matches.
  */
  /*********************************************************************************/
  
  /**********************************************************************************/
  /* program masala_merge : this program does a lev_merge and a reclink merge,
  and incorporates manually linked matches to merge two datasets.
  
  Required arguments:
  varlist: the variables defining the within-group match pools
           i.e. "state district" if you are looking to match villages only within districts
  s1: the match variable (must be the same in the master and using datasets)
  
  Optional arguments:
  idmaster: unique id for the master dataset, created internally as varlist-s1
            if not specified, in which case data must be unique on varlist-s1.
  idmaster: unique id for the using dataset, created internally as varlist-s1
            if not specified, in which case data must be unique on varlist-s1.
  listvars: any variables you want retained to help identify manual matches
  manual_file: the full filepath for the csv that contains manual matches
  csvsort: the variable on which you want to sort the csv of unmatched observations
  method: which methods to use in the merge; options are: levonly, rlonly (running
          lev_merge or reclink only). unspecified runs both
  nopreserve: if specified, will not preserve the original data (use for debugging)
  fuzziness: how much uncertainty to allow in the matches, used directly in lev_merge
             and used to generate thresholds for reclink (minscore and minbigram)
  minscore: reclink argument for match uncertainty threshold
            overrides fuzziness if specified, otherwise generated by fuzziness
  minbigram: reclink argument for weighting of string length in uncertainty threshold
             overrides fuzziness argument if specified, otherwise generated by fuzziness
  outfile, keepusing, sortwords: lev_merge arguments passed through
  */
  /**********************************************************************************/
  
  /**********************************************************************************/
  /* Manual matches work as follows:
  
  1. the manual links are entered through a csv file input as the `manual_file' argument.
  if this argument is left blank, no manual changes are incorporated.
  2. whether or not the `manual_file' is specified, masala_merge will output a file
  containing all of the non-matches to `unmatched_fn', which will
  contain the unique id's and also any variables specified in `listvars' from both the
  master and using datasets to help match observations using the identifying information.
  3. this file can then be opened and inspected by the user. the user can enter manual
  matches by filling in the idmatch field in the csv with the idusing they want to
  manually match to that idmaster.
  4. save the file, overwriting `unmatched_fn'.
  5. run the process_manual_matches program with `unmatched_fn' as
  the `new_file' argument to add these manual matches to the `manual_file'
  (or create the `manual_file' if there hasn't been one created for the merge).
  
  Note: each merge should only ever have one `manual_file'. `unmatched_fn'
  will be a temporary file for each iteration of masala_merge run for that merge.
  
  The program outputs the merged dataset, with the match_source variable identifying
  how (or if) the variable was matched. categories include: lev_merge, reclink,
  lev_merge & reclink, exact, manual, unmatched master, unmatched using.*/
  
  /***********************************************************************************/
  cap prog drop masala_merge
  prog def masala_merge
  {
    syntax [varlist] using/, S1(string)  ///
                             [idmaster(string) idusing(string) ///
                              LISTvars(varlist) MANUAL_file(string) CSVsort(string) METHOD(string) FUZziness(real 1.0) ///
                              MINSCORE(real 0.0)  MINBIGRAM(real 0.0) ///
                              OUTfile(string) KEEPUSING(passthru) nopreserve nonameclean] 
    
    quietly {
  
      /* preserve the initial data in case the program crashes, unless nopreserve is specified */
      if "`preserve'" == "" {
        preserve
      }

      /* create a random 5-6 digit number to make file names for this merge unique */
      local time = real(subinstr(`"`c(current_time)'"', ":", "", .))
      local nonce = floor(`time' * runiform() + 1)

      /* set default value for outfile */
      if mi("`outfile'") {
        tempfile outfile
      }
  
      /* set default value of csvsort */
      if mi("`csvsort'") local csvsort `s1'
      
      /* set default value for minscore based on fuzziness if not specified */
      if `minscore' == 0.0 {
        local minscore = 0.90 + (1 - `fuzziness') * 0.03 
      }
      
      /* set default value for minbigram based on fuzziness if not specified */
      if `minbigram' == 0.0 {
        local minbigram = 0.85 + (1 - `fuzziness') * 0.03
      }  
  
      /* if keepusing is specified, add the required s1 variable that must also be kept by the merge */
      if !mi("`keepusing'") {
        local keepusing = subinstr("`keepusing'", ")", " `s1'_using _`s1'_using `_varlist' `varlist' _rl_idu)", .)
      }
     
      /* ********************* */
      /* 1. Clean each dataset */
      /* ********************* */
  
      /* create merge versions of the relevant variables by adding a "_" prefix.
         these will be manipulated and used in the merge and then dropped in the final dataset. */
      local _varlist
      foreach var in `varlist' {
  
        /* force all identifiers to become strings */
        gen _`var' = `var'
        tostring _`var', replace force
        label var _`var' "`var', used in merge"
  
        /* create a version of varlist that contains the merge variables */
        local _varlist `_varlist' _`var'
      }
      
      /* name_clean the fuzzy merge string var */
      if "`nameclean'" == "" {
        name_clean `s1', generate(_`s1')
      }

      /* if nonameclean specified, simply generate _s1 */
      if "`nameclean'" == "nonameclean" {
        gen _`s1' = `s1'
      }
      label var _`s1' "`s1', used in merge"

      /* create an idmaster if it is missing */
      if "`idmaster'" == "" {
        egen idmaster = concat(`_varlist' `s1'), punct("-")
        local idmaster "idm"
      }

      /* store the original idmaster */
      local original_idmaster `idmaster'
      
      /* add the _master suffix onto the idmaster variable if it wasn't already added */
      if regexm("`idmaster'", "_master") == 0 {
        ren `idmaster' `idmaster'_master
        local idmaster `idmaster'_master
      }
  
      /* create a numeric identifier (required) for reclink */
      gen _rl_idm = _n
      label var _rl_idm "unique numeric id for the master data used in reclink"
      
      /* create a variable for the full group name */
      egen _group_name = concat(`_varlist'), punct(-)
      label var _group_name "group identifier to define within-group match pool"
  
      /* save master dataset with all observations and variables */
      tempfile master_all
      save `master_all'
  
      /* save another master dataset to be modified during the merge */
      tempfile master_working
      save `master_working'
      
      /* load the using dataset and perform the same steps */
      use `using', clear

      /* test if the dataset is unique- it must be because we are doing a m:1 merge */
      cap is_unique `varlist' `s1'
      if _rc != 0 {
          di as error "Error: the original using dataset is not unique on `varlist' `s1'."
          exit 9
      }
      
      /* create merge versions of the relevant variables by adding a "_" prefix.
         these will be manipulated and used in the merge and then dropped in the final dataset. */
      foreach var in `varlist' {
  
        /* force all identifiers to become strings */
        gen _`var' = `var'
        tostring _`var', replace force
        label var _`var' "`var', used in merge"
      }

      /* name_clean the fuzzy merge string var */
      if "`nameclean'" == "" {
        name_clean `s1', generate(_`s1')
      }    
      /* if nonameclean specified, simply generate _s1 */
      if "`nameclean'" == "nonameclean" {
        gen _`s1' = `s1'
      }
      label var _`s1' "`s1', used in merge"

      /* test if the using dataset is still unique after the name_clean- it must be because we are doing a m:1 merge */
      cap is_unique `_varlist' _`s1'
      if _rc != 0 {
          noi di as error "Error: after running name_clean on `s1', the using dataset is not unique on `_varlist' _`s1'"
          noi di as error "Either clean `s1' before running masala_merge, or run masala_merge specifying nonameclean."
          exit 9
      }

      /* create an idmaster if it is missing */
      if "`idusing'" == "" {
        egen idusing = concat(`_varlist' `s1'), punct("-")
        local idusing "idu"
      }

      /* store the original idusing */
      local original_idusing `idusing'
      
      /* add the _using suffix onto the variable if it isn't already there */
      if regexm("`idusing'", "_using") == 0 {
        ren `idusing' `idusing'_using
        local idusing `idusing'_using
      }
      
      /* create a numeric identifier (required) for reclink */
      gen _rl_idu = _n
      label var _rl_idu "unique numeric id for the using data used in reclink"
      
      /* create a variable for the full group name */
      egen _group_name = concat(`_varlist'), punct(-)
      label var _group_name "group identifier to define within-group match pool"
      
      /* save using dataset with all the variables and observations present */
      tempfile using_all
      save `using_all'
  
      /* save another using dataset to be modified during the merge */
      tempfile using_working
      save `using_working'
      
      /* reload the prepped master data */
      use `master_working', clear
      
      /* drop any groups that are not in both the master and using data */
      drop_unshared_groups using `using_working'
  
      /* save the master dataset with only the variables needed for the merge */
      keep `idmaster' `varlist' `_varlist' `s1' _`s1' `listvars' _group_name _rl_idm
      save `master_working', replace
      
      /* save the using dataset with only the variables needed for the merge */
      use `using_working', clear
      keep `idusing' `varlist' `_varlist' `s1' _`s1' `listvars' _group_name _rl_idu
      save `using_working', replace 
  
      /* ********************************************************** */
      /* 2. Merge the manual matches and extract them from the data */
      /* ********************************************************** */
  
      /* reload the master data */
      use `master_working', clear

      /* if the manual match file has been specified */
      if !mi("`manual_file'") {
  
        /* read in the manual match sheet */
        import delimited using `manual_file', varn(1) clear

        /* make sure idmaster and idusing are strings */
        tostring `idmaster', replace
        tostring `idusing', replace

        /* restrict dataset to the manual matches that occur in the master dataset */
        merge 1:1 `idmaster' using `master_working', keep(match) keepusing(`idmaster') nogen
        
        /* select the manual matches that occur in the using dataset */
        merge 1:1 `idusing' using `using_working', keep(match) keepusing(`idusing') nogen
  
        /* keep only the fields we need */
        keep `idmaster' `idusing' _`s1'_master _`s1'_using
        order `idmaster' `idusing' _`s1'_master _`s1'_using
  
        /* set the match source to be 5 (manual) */
        gen match_source = 5
        
        /* save the manual matches */
        tempfile manual_matches
        save `manual_matches'
      }
  
      /* if not specified, save an empty manual matches file with the required fields */
      else {
  
        /* save an empty manual matches file so we can proceed as if it exists */
        clear
        gen `idmaster'   = ""
        gen `idusing'    = ""
        gen _`s1'_master = ""
        gen _`s1'_using  = ""
        tempfile manual_matches
        save `manual_matches'
      }
      
      /* eliminate manual matches from master data */
      use `master_working', clear
      merge 1:1 `idmaster' using `manual_matches', keep(master) keepusing(`idmaster') nogen
      save `master_working', replace
  
      /* eliminate manual matches from using data */
      use `using_working', clear
      merge 1:1 `idusing' using `manual_matches', keep(master) keepusing(`idusing') nogen
      save `using_working', replace
  
      /* ******************************* */
      /* 3. Run lev_merge and reclink */
      /* ******************************* */
  
      /* run lev_merge unless rlonly is specified */
      if "`method'" != "rlonly" {
        
        /* open clean master data set */
        use `master_working', clear
        
        /* run lev_merge */
        lev_merge `_varlist' using `using_working', s(_`s1') outfile(`outfile') fuzziness(`fuzziness')

        /* check to see if the variable lev_dist exists, if it does not then no matches were found */
        cap confirm var lev_dist
        
        /* if there are matches, then rename variables */
        if _rc == 0 {

          /* rename variables to be levenshtein and master-specific */
          ren lev_dist _lev_dist
          ren `idusing' `idusing'_lev
          ren _`s1' _`s1'_master
        
          /* keep only the levenshtein matches */
          keep if _masala_merge == 3
        }

        /* if there is no lev_dist, clear the output because there are no matches */
        else {
          clear
        }
  
        /* check to see if there were any matches made */
        count
  
        /* if there were matches made, save them */
        if `r(N)' > 0 {
          /* keep only the variables that identify a match -- bring data in later */
          keep `idmaster' `idusing'_lev _`s1'_master _`s1'_using _lev_dist
          order `idmaster' `idusing'_lev _`s1'_master _`s1'_using _lev_dist
          
          /* save the matches */
          tempfile lev_matches
          save `lev_matches'
        }
  
        /* if there were no lev matches made, inlclude rlonly in the method to create a dummy levmatches file */
        else {
          local method "`method'rlonly"
        }
      }
  
      /* run reclink unless levonly is specified */
      if "`method'" != "levonly" {
        
        /* reload master dataset */
        use `master_working', clear

        /* call reclink */
        noi reclink `_varlist' _`s1' using `using_working', idm(_rl_idm) idu(_rl_idu) gen(_reclink_dist_raw) required(`_varlist') minscore(`minscore') minbigram(`minbigram')
  
        save $tmp/debug_test, replace
        /* keep only those merged according to the minimum score */
        drop if _merge != 3
        drop _merge
  
        /* check to make sure there were matches made */
        count
  
        /* if there were, save them in the rl_matches file */
        if `r(N)' > 0 {
          /* rescale the reclink_dist measure */
          sum _reclink_dist_raw
          
          /* normalize the score 0 -1 */
          gen _reclink_dist = (_reclink_dist_raw - `r(min)') / (`r(max)' - `r(min)')
  
          /* swap the scale so that a score of 0 is the best match (instead of 1, as reclink outputs) */
          replace _reclink_dist = (_reclink_dist * -1) + 1
  
          /* multiply by 2 to match the scale used by lev_merge */
          replace _reclink_dist = _reclink_dist * 2
        
          /* rename variables to mimic the lev_merge format */
          ren `idusing' `idusing'_rl
          ren _`s1' _`s1'_master
          ren U_`s1' _`s1'_using
        
          /* drop duplicates where multiple idmasters are linked to the same idusing -
          these are unknown matches */
          ddrop `idmaster'
  
          /* save results in temp file */
          keep `idmaster' `idusing'_rl _`s1'_master _`s1'_using _reclink_dist_raw _reclink_dist
          order `idmaster' `idusing'_rl _`s1'_master _`s1'_using _reclink_dist_raw _reclink_dist
          tempfile rl_matches
          save `rl_matches'
        }
  
        /* if there were no reclink matches made, inlclude levonly in the method to create a dummy rlmatches file */
        else {
          local method "`method'levonly"
        }
      }
      
      /* create empty match tempfiles for the other merges if levonly or rlonly was specified */
      foreach m in rl lev {
        
        /* if the method dictates rlonly or levonly, create the empty file for it */
        if strpos("`method'", "`m'") != 0 {
  
          clear
          
          /* if the method is rlonly, create a dummy lev_matches file */
          if "`m'" == "rl" {
            local m_prime lev
            gen _`m_prime'_dist = .
          }
  
          /* if the method is levonly, create a rl_matches file */
          if "`m'" == "lev" {
            local m_prime rl
            gen _reclink_dist = .
          }
  
          /* create empty fields that are required by the combination code below */
          gen `idmaster' = ""
          gen `idusing'_`m_prime' = ""
          gen _`s1'_master = ""
          gen _`s1'_using = ""
          
          /* save the otherwise empty file in `lev_matches' or `rl_matches' */
          tempfile `m_prime'_matches
          save ``m_prime'_matches', emptyok replace
        }
      }
      
      /* ********************************************************** */
      /* 4. Combine the matches from lev_merge, reclink, and manual */
      /* ********************************************************** */
  
      /* reload the reclink matches */
      use `rl_matches', clear
      
      /* create correct matches dataset by merging reclink and masala matches */
      /* note: both of these datasets have _s1_master and _s1_using,
               which at first seems fraught. Stata will keep only the RL version.
               But note below that we drop if RL and LEV matched these to different
               individuals. */
      merge 1:1 `idmaster' using `lev_matches'
  
      /* create the match_source variable and fill with proper values */
      gen match_source = 2
      replace match_source = 3 if _merge == 2
      replace match_source = 4 if _merge == 1
      drop _merge
      label var match_source "how each match was made (or not made) in the merge"
      label define match_source  1 "1 exact string match" 2 "2 lev & rl" 3 "3 lev only" 4 "4 rl only" 5 "5 manual" 6 "6 unmatched master" 7 "7 unmatched using"
      label values match_source match_source
        
      /* drop any matches that are disputed between lev and reclink--
         these are considered unmatched */
      drop if !mi(`idusing'_lev) & !mi(`idusing'_rl) & `idusing'_lev != `idusing'_rl
     
      /* generate one idusing, either from lev_merge or reclink
         (order doesn't matter as we dropped those that don't match) */
      gen `idusing' = `idusing'_lev
      replace `idusing' = `idusing'_rl if mi(`idusing')
  
      /* use the match distance corresponding to the lower of the two methods */
      gen masala_dist = min(_lev_dist, _reclink_dist)
      label var masala_dist "overall measure of match distance"
  
      /* at this point we can have a master id linked to different using id's with different match_source*/
      /* ddrop matches in which reclink and lev_merge matched the same idusing to different idmaster */
      // ddrop `idusing'
  
      /* keep only the necessary columns */
      keep `idmaster' `idusing' _`s1'_master _`s1'_using match_source masala_dist _reclink_dist _lev_dist
    
      /* replace match source if idmaster and idusing are an exact string match */
      replace match_source = 1 if _`s1'_master == _`s1'_using
  
      /* append the manual matches -- this file is empty if there are no manual matches */
      append using `manual_matches'
              
      /* order and save all the matches automatically matched by masala and reclink */
      order `idmaster' `idusing' _`s1'_master _`s1'_using match_source masala_dist
      save $tmp/all_matches_`nonce', replace
      
      /* *************************************************************************** */
      /* 5. Merge in the unmatched observations along with the rest of the variables */
      /* *************************************************************************** */
      
      /* Load all the master data */
      use `master_all', clear
  
      /* rename the s1 variable to distinguish it from the using side, which has the same variable */
      ren `s1' `s1'_master
      ren _`s1' _`s1'_master
  
      /* get a list of all the variables in the master dataset */
      qui describe, varlist
      local mastervars "`r(varlist)'"
  
      /* merge the master data into the match data */
      merge 1:1 `idmaster' using $tmp/all_matches_`nonce'
      ren _merge _m1
  
      /* save as a tempfile */
      tempfile all_master
      save `all_master'
  
      /* Load all using data */
      use `using_all', clear
  
      /* rename the using listvars from using to compare them to the master values later */
      local listvars_using
      foreach var in `listvars' {
        local listvars_using `listvars_using' `var'_u
        ren `var' `var'_u
      }
  
      /* rename the varlist from using to compare them to the master varlist later */
      foreach var in `varlist' {
        ren `var' `var'_u
      }
      
      /* rename the s1 variable to distinguish it from the master side, which as the same variable */
      ren `s1' `s1'_using
      ren _`s1' _`s1'_using
  
      /* get all the variables in the using dataset */
      qui describe, varlist
      local usingvars "`r(varlist)'"
  
      /* save all the using data in a tempfile */
      tempfile all_using
      save `all_using'
  
      /* reload the matches + unmatched master dataset */
      use `all_master', clear
  
      /* merge in with all the using data */
      merge m:1 `idusing' _group_name using `all_using', update `keepusing'
      ren _merge _m2
      
      /* label match source as unmatched from the master and using sides */
      replace match_source = 6 if _m1 == 1
      replace match_source = 7 if _m2 == 2
      drop _m1 _m2
  
      if mi("`keepusing'") {
        /* identify the overlapping variables between master and using */
        local both : list mastervars & usingvars
        local both : list sort both
  
        noi disp_nice "WARNING: The following variables appear in both datasets. The master value is selected."
        noi di "shared variables: `both'"
      }
      
      /* save the merge results using this merge's unqiue 6-digit identifier */
      save $tmp/merge_results_`nonce', replace
  
      /* ************************* */
      /* 6. Output the non-matches */
      /* ************************* */
  
      /* write non-matches to a csv */
      keep if match_source == 6 | match_source == 7
  
      /* if there are unmatched observations, report them in a csv */
      local unmatched_count = _N
  
      if `unmatched_count' != 0 {
  
        /* to sort by s1, create a _sortby variable that combines s1_using and s1_master */
        if  "`csvsort'" == "`s1'" {
          /* make _sortby a combination of s1_master and s1_using */
          gen _sortby = `s1'_master
          replace _sortby = `s1'_using if mi(`s1'_master) 
        }
  
        /* else if csvsort is another variable, create _sortby */
        else {
          /* combine the master and using (_u) versions of the csvsort variable */
          gen _sortby = `csvsort'
          replace _sortby = `csvsort'_u if mi(_sortby)
        }
  
        /* sort the csv */
        sort _group_name _sortby
        drop _sortby
  
        /* put master and using `listvars' in the same column to cut down on screen space in the csv */
        foreach var in `listvars' {
          replace `var' = `var'_u if mi(`var')
          drop `var'_u
        }
   
        /* keep only the variables we're interested in */
        keep `_varlist' _`s1'_master `listvars' _`s1'_using `idusing' `idmaster' _group_name
        save $tmp/debug, replace
        
        /* drop _group_names that have only idusing or idmaster (meaning there are no possible matches) */
        foreach v in master using {
          gen tmp = !mi(_`s1'_`v')

          /* check if there are some observations that are not missing the matched variabel */
          qui sum tmp

          /* only proceed if there are some */
          if `r(N)' != 0 {
            egen _has_`v'  = max(tmp), by(_group_name)
            keep if _has_`v' == 1
            drop _has_`v'
          }
          drop tmp
        }
  
        /* create a local variable to be the match id column */
        local idmatch = subinstr("`idusing'", "using", "match", .)
  
        /* if there are no unmatched observations, print a message */
        if _N == 0 {
          noi di "No unmatched observations are in overlapping groups, therefore no manual match file will be exported."
        }
  
        /* else if ther are unmatched observations, output them as a csv */
        else {
          /* insert an empty column where the user can specify manual matches */
          gen `idmatch' = ""
          
          /* compress to makse all string variables the proper length */
          compress
          
          /* insert lines between groups and then drop _group_name for brevity */
          insert_group_lines
          drop _group_name
  
          /* save the unmatched observations with the unique 6-digit identifier */
          local unmatched_fn = "$tmp/unmatched_observations_`nonce'.csv"
  
          /* export unmatched observations as a csv */
          order `_varlist' `listvars' _`s1'_master _`s1'_using `idusing' `idmaster' `idmatch'
          export delimited `unmatched_fn', replace
        }
      }
      
      /* reload the final output */
      use $tmp/merge_results_`nonce', clear
      
      /* drop the variables created for the merge */
      drop _rl_idm _rl_idu _reclink_dist _lev_dist `_varlist' _group_name _`s1'_master _`s1'_using
      
      /* create a _merge variable to mimic stata's merge results */
      gen _merge = 3
      replace _merge = 2 if match_source == 7
      replace _merge = 1 if match_source == 6
      label define _merge 1 "1 master" 2 "2 using" 3 "3 match"
      label values _merge _merge
      
      /* save the select variables for all the matches */
      order `idmaster' `idusing' `s1'_master `s1'_using
      save $tmp/merge_results_`nonce', replace
  
      /* drop unmatched results and sort */
      drop if match_source == 6 | match_source == 7
      gsort -masala_dist
  
      /* display the worst matches */
      drop if match_source == 1
  
      /* sort all the listvars so that the master and using values versions are next to each other */
      local listvars_all : list listvars | listvars_using
      local listvars_all : list sort listvars_all
  
      /* if there are fewer than 25 high cost matches, print all of them  */
      if _N < 25 {
        noi list `s1'_master `s1'_using `listvars_all' match_source masala_dist 
      }
  
      /* else print the 25 highest cost matches */
      else {
        noi list `s1'_master `s1'_using `listvars_all' match_source masala_dist in 1/25
      }
      
      /* reload final result to memory */
      use $tmp/merge_results_`nonce', clear 
  
      /* if the original idusing and idmaster variables are not the same, restore them (removing the _master and _using tages) */
      if "`original_idusing'" != "`original_idmaster'" {
        ren `idusing' `original_idusing'
        ren `idmaster' `original_idmaster'
      }
  
      /* combnine the _u with the master variables into one column */
      foreach var in `varlist' {
        replace `var' = `var'_u if mi(`var')
        drop `var'_u
      }
       
      save $tmp/merge_results_`nonce', replace
      
      /* overwrite prerserved data with results */
      if "`preserve'" == "" {
        tempfile final_data
        save `final_data', replace
        restore
        use `final_data', clear
      }
      
      /* display merge results */
      noi disp_nice "MASALA_MERGE RESULTS"
      noi tab match_source
      noi di ""
      noi di "Final returned merge data is also stored at :  $tmp/merge_results_`nonce'"
      
      /* if there were unmatched observations output, display information about adding manual matches */
      if `unmatched_count' != 0 {
        noi di "Unmatched observations were output here: `unmatched_fn'"
        noi di ""
        noi di "To add more manual matches from the unmatched observations:"
        noi di "1. Open the file, manually match master and using pairs by inserting a unique idusing into the proper row in the idmatch column."
        noi di "2. Save the file, run process_manual_matches with the new match file and the master match file for this merge."
        noi di "   --> process_manual_matches, outfile(string) infile(string) S1(string) idmaster(string) idusing(string) [charsep(string)]"
        noi di "     ex. process_manual_matches, outfile(\$stringdir/within_state_cand_ts_matches.csv) infile(`unmatched_fn') s1(`s1') idmaster(`idmaster') idusing(`idusing')"
        noi di "3. Re-run masala_merge or just run insert_manual_matches with the now updated master manual match file specified."
      }
  
      /* display a message if there are no unmatched observations */
      else {
        noi disp_nice "There are no unmatched observations, this was a perfect 1:1 match."
      }
    }
  }
  end
  /* *********** END program masala_merge ***************************************** */
  
  
  /**********************************************************************************/
  /* program process_manual_matches : this program prepares the new matches the user
  entered in the `infile' csv file to be included in masala_merge. if there is
  already a master manual matches file for this merge (`outfile'), this program
  adds the new matches, checks for duplicate matches, and resaves the `outfile'
  csv. if `outfile' does not exist yet, this program saves the new matches to a
  new `outfile'.
  
  Example:
  
  process_manual_matches, outfile(candidate_ts_within_state_match.csv) infile(`unmatched_fn')
       --> this will add the manually entered matches from `unmatched_fn' to the outfile so they
           will always be included when this particular merge is done with masala_merge
  */
  /***********************************************************************************/
  cap prog drop process_manual_matches
  prog def process_manual_matches
  
  {
    syntax, outfile(string) infile(string) S1(string) idmaster(string) idusing(string) [charsep(string)]
  
    quietly {
  
      /* import the new manual matches */
      import delimited using `infile', varn(1) clear
  
      /* define the character that creates the group separator lines */
      if mi("`charsep'") {
        local charsep  "-"
      }
      
      /* ensure that the idmaster variable has the _master suffix*/
      if regexm("`idmaster'", "_master") == 0 {
        local idmaster `idmaster'_master
      }
  
      /* ensure that the idusing variable has the _using suffix */
      if regexm("`idusing'", "_using") == 0 {
        local idusing `idusing'_using
      }
  
      /* set the idmatch variable by substituting _match for _using in the suffix */
      local idmatch = subinstr("`idusing'", "_using", "_match", .)
  
      /* FIRST: Get the s1_using names to fill the new matches, because the match is only defined
         using the id's, we need to get the s1 names that correspond to the new idmatch matches. */
      keep `idusing' _`s1'_using
  
      /* drop the group separator lines by defining a line that starts and ends with
      a double `charsep' character as being a group separator line. */
      drop if regexm(`idusing', "`charsep'`charsep'$") & regexm(`idusing', "^`charsep'`charsep'")
  
      /* keep only the using side */
      drop if mi(`idusing')
      
      /* rename the idusing to be idmatch to merge in the s1 names below */
      ren `idusing' `idmatch'
      
      /* save all the using id's and the s1_using names to be merged in later */
      tempfile newmatch_names
      save `newmatch_names'
      
      /* SECOND: extract all the new matches */
      import delimited using `infile', varn(1) clear
      
      /* drop all other variables */
      keep `idmaster' `idmatch' _`s1'_master _`s1'_using
      order `idmaster' `idmatch' _`s1'_master _`s1'_using
      
      /* keep only the rows that contain new matches */
      keep if !mi(`idmaster') & !mi(`idmatch')
  
      /* drop the group separator lines */
      drop if regexm(`idmatch', "`charsep'`charsep'$") & regexm(`idmatch', "^`charsep'`charsep'")
  
      /* drop s1_using names, which are all blank, to be filled */
      drop _`s1'_using
      
      /* merge in the s1_using names, keeping only the names for new matches */
      merge 1:1 `idmatch' using `newmatch_names', keep(match)  nogen
  
      /* rename idmatch to be idusing for the final output file */
      ren `idmatch' `idusing'
      
      /* save as temporary file */
      tempfile new_matches
      save `new_matches'
      
      /* check if the manual match file exists */
      cap confirm file "`outfile'"
      
      /* if it does, add the new matches to it */
      if _rc == 0 {
  
        /* read in the manual match file */
        import delimited using "`outfile'", varn(1) clear
  
        /* append the new matches to the existing matches */
        append using `new_matches', force
  
        /* drop duplicates of repeated matches */
        duplicates drop
  
        /* check for any conflicting matches, where one idmaster is linked to different idusing's */
        drep `idmaster'
        local dup = `r(N)' - `r(unique_value)'
  
        /* throw error and halt program if there are conflicting duplicates */
        if `dup' != 0 {
          noi dreps `idmaster', list(`idmatch' _`s1'_master _`s1'_using)
          di as error "Error: the above matches are conflicting between the existing and the new files."
          exit 9
        }
      }

      /* export all manual matches to the manual match file */
      export delimited "`outfile'", replace
    }
  }
  end
  /* *********** END program process_manual_matches ***************************************** */
  
  
  /**********************************************************************************/
  /* program insert_group_lines : this program inserts a separating line between
  the different groups in the mismatch csv file output by masala_merge. matches must
  be made within groups, so the line visually aids the manual match process. data
  must be sorted by _group_name before this program is called.
  
  It works by identifying the rows where the group changes and inserting a numerical
  jump in the index there, then creating rows filled with a particular character
  at the now skipped index values, appending those rows onto the main dataset, and sorting
  by the numerical index to put the lines at the proper rows.
  
  grp_name: the variable defining the groups
  charsep: the character to be written to define the line
  
  Example:
  insert_group_lines, grp_name(_group_name) charsep("^") 
      --> result: a row filled with "^" are inserted between each unique _group_name
  */
  /***********************************************************************************/
  cap prog drop insert_group_lines
  prog def insert_group_lines
  {
    syntax, [GRP_name(string) CHARsep(string)]
  
    quietly {
  
      /* set default value of grp_name, charsep, numlines*/
      if mi("`grp_name'") local grp_name _group_name
      if mi("`charsep'") local charsep "-"
      
      /* create numbered index */
      gen index  = _n
  
      /* identify where the group name changes */
      egen _change = tag(`grp_name')
  
      /* calculate the cumulative sum of the _change tag */
      gen _cum_change = 0
      replace _cum_change = _cum_change[_n-1] + _change[_n] if _n > 1
  
      /* add the cumulative change to the index, this inserts a skip at the index variable indicated by _change */
      replace index = index + _cum_change
      drop _cum_change
      
      /* save file temporarily to come back to */
      tempfile csv_temp
      save `csv_temp'
  
      /* isolate the rows where the _group_name changes */
      drop if _change == 0
  
      /* subtract one to get the missing index number where the line needs to be inserted */
      replace index = index - 1 
  
      /* get a list of all the variables in the dataset */
      ds
      local all_vars = "`r(varlist)'"
  
      /* cycle through each variable, replacing its value with `charsep' */
      foreach var in `all_vars' {
  
        /* don't replace the index variable, this is needed for matching with the original data */
        if "`var'" != "index" {
  
          /* if it is a string variable, replace with the string `charsep' */
          if strpos("`: type `var''", "s") != 0 {
          
            /* get the number of characters in the variable from the storage type */
            local vartype: type `var'
            local numchar = subinstr("`vartype'", "str", "", .)
  
            /* compare the length in storage type to the length of the variable name itself, take the max */
            local numchar = max(`numchar', length("`var'"))
  
            /* initialize the linefill string  */
            local linefill "`charsep'"
  
            /* append `charsep' until `linefill' is the full length of the variable */
            forvalues i = 1(1)`numchar' {
              local linefill = "`linefill'" + "`charsep'"
            }
  
            /* replace the variable with the `linefill' */
            replace `var' = "`linefill'"         
          }
  
          /* else if the variable is not a string, clear it */
          else {
            replace `var' = .
          }
        }
      }
      
      /* append the skipped lines (filled with `charsep') to the original data */
      append using `csv_temp'
  
      /* sort by the index to put the skipped lines in the correct place */
      sort index
      
      /* drop unneeded variables */
      drop index _change
   }  
  }
  end
  /* *********** END program insert_group_lines ***************************************** */
  
  /**************************************************************************************/
  /* program drop_unshared_groups :  this is a helper program for masala_merge.
     It drops groups that don't appear in both the master and using datasets from both. */
  /**************************************************************************************/
  cap prog drop drop_unshared_groups
  prog def drop_unshared_groups
  {
    syntax using/
  
    quietly {
      
      /* save the filename of the open dataset to be used later */
      local master = "`c(filename)'"
      
      /* keep only the _group_name variable */
      keep _group_name
  
      /* drop all duplicates */
      duplicates drop
  
      /* save all the unique groups in the master data to a temporary file */
      tempfile master_groups
      save `master_groups', replace
  
      /* open the using data */
      use `using', clear
         
      /* get all unique groups present in the using dataset */
      keep _group_name
      duplicates drop
  
      /* merge to the list of groups in the master dataset */
      merge 1:1 _group_name using `master_groups'
  
      /* save the list of matches only */
      keep if _merge == 3
      drop _merge
      tempfile matched_groups
      save `matched_groups', replace
  
      /* keep only matched groups on the using side */
      use `using', clear
      merge m:1 _group_name using `matched_groups', keep(match) nogen
      save `using', replace
  
      /* keep only matched groups on the master side */
      use `master', clear
      merge m:1 _group_name using `matched_groups', keep(match) nogen
      save `master', replace
    }
  }
  end
  /* *********** END program drop_unshared_groups ***************************************** */
  
  
  /**********************************************************************************/
  /* program insert_manual_matches  : update a merged dataset with new manual matches.
     this program is split off from masala_merge to allow adding new manual matches to
     a merged file without having to run lev_merge and reclink again. this should
     be run on the final merge result returned by masala_merge. */
  /***********************************************************************************/
  
  cap prog drop insert_manual_matches 
  prog def insert_manual_matches 
  {
  
    syntax, manual_file(string) idmaster(string) idusing(string) [nopreserve]

    /* preserve the initial data in case the program crashes, unless nopreserve is specified */
    if "`preserve'" == "" {
      preserve
     }
    
    quietly {
      /* ensure the idmaster variable has the _master suffix */
      if regexm("`idmaster'", "_master") == 0 {
        local idmaster `idmaster'_master
      }
      
      /* ensure the idusing variable has the _using suffix */
      if regexm("`idusing'", "_using") == 0 {
        local idusing `idusing'_using
      }
  
      /* drop masala_merge _merge variable */
      cap drop _merge
  
      /* save as a temp stata file */
      tempfile merge_results
      save `merge_results'
      
      /* read in the manual match sheet */
      import delimited using `manual_file', varn(1) clear

      /* make sure idmaster and idusing are strings */
      cap tostring `idmaster', replace
      cap tostring `idusing', replace

      /* save as a temp stata file */
      tempfile manual_csv
      save `manual_csv'
  
      /* get the unmatched master results */
      use `merge_results', clear
      drop if  match_source != 6
      tempfile master_results
      save `master_results'
  
      /* get the unmatched using results */
      use `merge_results', clear
      drop if match_source != 7
      tempfile using_results
      save `using_results'
      
      /* open the manual matches */
      use `manual_csv', clear
  
      /* merge in the new matches based on the idmaster */
      merge 1:1 `idmaster'  using `master_results', keep(match) nogen
  
      /* merge in the variables from the using results, updating missing fields */
      merge 1:1 `idusing' using `using_results', update
  
      /* drop nonmatched entries */
      drop if _merge < 3
      drop _merge
  
      /* replace match_source with manual for all of these matches */
      replace match_source = 5
  
      /* create flg to indicate these are new manual matches */
      gen _new_match_flg = 1
      
      /* save temp file */
      tempfile new_matches
      save `new_matches'
  
      /* reopen the merge results and append the new matches */
      use `merge_results', clear
      append using `new_matches'
  
      /* drop duplicates of exactly repeated matches */
      duplicates drop
      
      /* tag the duplicated idmasters, the id's to be replaces and the incoming id's to replace them. */
      duplicates tag `idmaster', gen(_dup_master)
  
      /* identify any existing matches that will be overwritten by incoming manual matches */
      count if match_source != 6 & match_source !=7 & _dup_master != 0 & !mi(`idmaster')
  
      /* if any of these duplicated idmasters are NOT unmatched, warn that they will be overwritten by the incoming manual match */
      if `r(N)' != 0 {
        noi list `idmaster' `idusing' if match_source != 6 & match_source !=7 & _dup_master != 0 & !mi(`idmaster')
        noi di "Warning the above existing matches will be overwritten by manual matches in the csv."
      }
      
      /* drop the exsting matches that are now replaced with incoming new manual matches */
      drop if _dup_master != 0 & !mi(`idmaster') & _new_match_flg != 1
  
      /* drop unmatched using entries that now have manual matches */
      duplicates tag `idusing', gen(_dup_using)
      drop if _dup_using != 0 & !mi(`idusing')  & _new_match_flg != 1 & match_source == 7
  
      /* drop temporary columns used in merge */
      drop _dup_using _dup_master
  
      /* re-create the masala_merge _merge variable to mimic stata's merge results */
      gen _merge = 3
      replace _merge = 2 if match_source == 7
      replace _merge = 1 if match_source == 6
      label define _merge 1 "1 master" 2 "2 using" 3 "3 match"
      label values _merge _merge
  
      noi disp_nice "MASALA-MERGE RESULTS"
      noi tab match_source
    }  
  }
  end
  /* *********** END program insert_manual_matches  ***************************************** */
    
  /**********************************************************************************/
  /* program lev_merge : Fuzzy match using masalafied levenshtein                */
  /*
  Meta-algorithm:
  1. stata outsheets two files, with an id and a name column.
  2. python gets filenames and parameters from command lines, reads two files into two dictionaries
  3. python outputs a single file, with id, str1, str2, distance
  4. stata reads this file and processes, makes decisions about which matches to keep
  
  See readme.md for sample usage.
  
  */
  /***********************************************************************************/
  cap prog drop lev_merge
  prog def lev_merge
  {
    syntax [varlist] using/, S1(string) [OUTfile(string) FUZZINESS(real 1.0) quietly KEEPUSING(passthru) SORTWORDS] 
  
    /* drop lev_dist if it already exists */
    cap drop lev_dist
    
    /* require tmp and masala_dir folders to be set */
    if mi("$tmp") | mi("$MASALA_PATH") {
        disp as error "Need to set globals 'tmp' and 'MASALA_PATH' to use this program"
        exit
    }
  
    /* store sort words parameter */
    if !mi("`sortwords'") {
      local sortwords "-s"
    }
    
    /* define maximum distance for lev.py as 0.35 + 1.25 * (largest acceptable match).
       This is the threshold limit, i.e. if we accept a match at 2.1, we'll reject it
          if there's another match at 0.4 + 2.1*1.25. (this is hardcoded below) */
    local max_dist = 0.40 + 1.25 * 2.1 * `fuzziness'
    
    /* make everything quiet until python gets called -- this output is not helpful */
    qui {
  
      /* if no outfile is specified, create a temporary one */
      if mi("`outfile'") {
        tempfile outfile
      }
      
      /* create temporary file to store original dataset */
      tempfile master
      save `master', replace
  
      /* create a random 5-6 digit number to make the temp files unique */
      local time = real(subinstr(`"`c(current_time)'"', ":", "", .))
      local nonce = floor(`time' * runiform() + 1)
      
      local src1 $tmp/src1_`nonce'.txt
      local src2 $tmp/src2_`nonce'.txt
      local out $tmp/out_`nonce'.txt
      local lev_groups $tmp/lev_groups_`nonce'.dta
      
      preserve
      
      keep `varlist' `s1'
      sort `varlist' `s1'
      
      /* merge two datasets on ids to produce group names */
      merge m:m `varlist' using `using', keepusing(`varlist' `s1')
      
      // generate id groups
      egen g = group(`varlist')
      drop if mi(g)
      
      qui sum g
      local num_groups = r(max)
              
      // save group list
      keep g `varlist'
      duplicates drop
      save "`lev_groups'", replace
      
      /* now prepare group 1 */
      restore
      preserve
      
      keep `varlist' `s1'
    
      /* drop if missing string and store # observations */
      keep if !mi(`s1')
      qui count
      local g1_count = r(N)
      
      /* bring in group identifiers */
      merge m:1 `varlist' using "`lev_groups'", keepusing(g)
    
      /* places with missing ids won't match group */
      drop if _merge == 1
    
      /* only keep matches */
      keep if _merge == 3
      duplicates drop
      
      // outsheet string group 1
      outsheet g `s1' using "`src1'", comma replace nonames
      
      // prepare group2
      di "opening `using'..."
      use `using', clear
      keep `varlist' `s1'
  
      /* confirm no duplicates on this side */
      tempvar dup
      duplicates tag `varlist' `s1', gen(`dup')
      count if `dup' > 0
      if `r(N)' > 0 {
        display as error "`varlist' `s1' not unique on using side"
        exit 123
      }
      drop `dup'
      
      /* drop if missing string and store # observations */
      keep if !mi(`s1')
      qui count
      local g2_count = r(N)
      
      // merge in group identifiers
      merge m:1 `varlist' using "`lev_groups'", keepusing(g)
      
      /* something wrong if didn't match group ids for any observation */
      drop if _merge == 1
    
      /* only keep matches */
      keep if _merge == 3
      duplicates drop
      
      // outsheet string group 2
      outsheet g `s1' using "`src2'", comma replace nonames
    }
    
    // call python levenshtein program
    di "Matching `g1_count' strings to `g2_count' strings in `num_groups' groups."
    di "Calling lev.py:"
  
    di `" shell python -u $MASALA_PATH/lev.py -d `max_dist' -1 "`src1'" -2 "`src2'" -o "`out'" `sortwords'"'
    shell python $MASALA_PATH/lev.py -d `max_dist' -1 "`src1'" -2 "`src2'" -o "`out'" `sortwords'
  
    di "lev.py finished."
  
    /* quietly process the python output */
    qui {
      /* open output lev dataset */
      /* take care, this generates an error if zero matches */
      capture insheet using "`out'", comma nonames clear
    
      /* if there are zero matches, create an empty outfile and we're done */
      if _rc {
        disp_nice "WARNING: lev_merge: There were no matches. Empty output file will be saved."
        clear
        save `outfile', replace emptyok
        exit
      }
      ren v1 g
      ren v2 `s1'_master
      ren v3 `s1'_using
      ren v4 lev_dist
    
      /* merge group identifiers back in */
      destring g, replace
      merge m:1 g using "`lev_groups'", keepusing(`varlist')
      
      /* _m == 1 would imply that our match list has groups not in the initial set */
      assert _merge != 1
    
      /* _m == 2 are groups with zero matches. drop them */
      drop if _merge == 2
    
      /* count specificity of each match */
      bys g `s1'_master: egen master_matches = count(g)
      bys g `s1'_using: egen using_matches = count(g)
    
      /* count distance to first and second best match */
      foreach v in master using {
  
        /* rank the matches based on lev_dist */
        bys g `s1'_`v': egen `v'_dist_rank = rank(lev_dist), unique
        
        /* calculate best match for each var */
        gen tmp = lev_dist if `v'_dist_rank == 1
        bys g `s1'_`v': egen `v'_dist_best = max(tmp)
        drop tmp
  
        /* calculate the second best match */
        gen tmp = lev_dist if `v'_dist_rank == 2
        bys g `s1'_`v': egen `v'_dist_second = max(tmp)
        drop tmp
        
        drop `v'_dist_rank
      }
      
      drop g _m
    
      /* apply optimal matching rule (based on 1991-2001 pop census confirmed village matches in calibrate_fuzzy.do) */
      /* initialize */
      gen keep_master = 1
      gen keep_using = 1
    
      /* get mean length of matched string */
      gen length = floor(0.5 * (length(`s1'_master) + length(`s1'_using)))
    
      /* 1. drop matches with too high a levenshtein distance (threshold is a function of length) */
      replace keep_master = 0 if lev_dist > (0.9 * `fuzziness') & length <= 4
      replace keep_master = 0 if lev_dist > (1.0 * `fuzziness') & length <= 5
      replace keep_master = 0 if lev_dist > (1.3 * `fuzziness') & length <= 8
      replace keep_master = 0 if lev_dist > (1.4 * `fuzziness') & inrange(length, 9, 14)
      replace keep_master = 0 if lev_dist > (1.8 * `fuzziness') & inrange(length, 15, 17)
      replace keep_master = 0 if lev_dist > (2.1 * `fuzziness')
      
      /* copy these thresholds to keep_using */
      replace keep_using = 0 if keep_master == 0
    
      /* 2. never use a match that is not the best match */
      replace keep_master = 0 if (lev_dist > master_dist_best) & !mi(lev_dist)
      replace keep_using = 0 if (lev_dist > using_dist_best) & !mi(lev_dist)
      
      /* 3. apply best empirical safety margin rule */
      replace keep_master = 0 if (master_dist_second - master_dist_best) < (0.4 + 0.25 * lev_dist)
      replace keep_using = 0 if (using_dist_second - using_dist_best) < (0.4 + 0.25 * lev_dist)
    
      /* save over output file */
      order `varlist' `s1'_master `s1'_using lev_dist keep_master keep_using master_* using_*
      save `outfile', replace
    }
    restore
  
    /* run lev_review */
    use `outfile', clear
    
    /* if quietly is not specified, call lev_review, which calls lev_process */
    if mi("`quietly'") {
      lev_review `varlist', s1(`s1') master(`master') using(`using')
    }
  
    /* if quietly is specified, go directly to lev_process */
    else {
      lev_process `varlist', s1(`s1') master(`master') using(`using')
    }
    
    di "Masala-Levensthein merge complete."
    di " Original master file was saved here:   `master'"
    di " Complete set of fuzzy matches is here: `outfile'"
  }
  end
  /* *********** END program lev_merge ***************************************** */
  
  /**********************************************************************************/
  /* program masala_lev_dist : Calculate levenshtein distance between two vars */
  /*                           uses external python program */
  /***********************************************************************************/
  cap prog drop masala_lev_dist
  prog def masala_lev_dist
  {
    syntax varlist(min=2 max=2), GEN(name)
    tokenize `varlist'
    foreach i in _masala_word1 _masala_word2 _masala_dist __masala_merge {
      cap drop `i'
    }
  
    gen _masala_word1 = `1'
    gen _masala_word2 = `2'
    replace _masala_word1 = lower(trim(_masala_word1))
    replace _masala_word2 = lower(trim(_masala_word2))
  
    gen _row_number = _n
    
    /* create temporary file for python  */
    outsheet _row_number _masala_word1 _masala_word2 using $tmp/masala_in.csv, comma replace nonames
  
    /* call external python program */
    di "Calling lev.py..."
    shell python $MASALA_PATH/lev.py -1 $tmp/masala_in.csv -o $tmp/masala_out.csv
  
    /* convert created file to stata format */
    preserve
    insheet using $tmp/masala_out.csv, clear names
    save $tmp/masala_lev_dist, replace
    restore
  
    /* merge result with new dataset */
    merge 1:1 _row_number using $tmp/masala_lev_dist.dta, gen(__masala_merge) keepusing(_masala_dist)
  
    /* clean up */
    destring _masala_dist, replace
    ren _masala_dist `gen'
    drop _masala_word1 _masala_word2 _row_number
    
    assert __masala_merge == 3
    drop __masala_merge
  }
  end
  /* *********** END program masala_lev_dist ***************************************** */
  
  /*****************************************************************************************************/
  /* program fix_spelling : Fixes spelling in a string variable based on a supplied master key.        */
  /*                Runs a fuzzy lev-merge of data-list to master-list within group if specified    */
  
  /* Process: take data in data_list, merge it to master list
              keep if _m == 2
              fuzzy merge data-list to master-list, maybe within some group.
              if a single match without competition, then replace data list with the data in master list
              return new version of data list                                                          */
  /* Must specify gen() or replace                                                                     */
  
  /* Targetfield and targetgroup() allow varnames in the master set to differ from varnmes in the key  */
  /* also allow running of program for pc01_district_name if a variable pc01_district_name already exists
     in set
  targetfield:  refers to variable name in key named according to pc`year'_`place'_name
  targetgroup:  refers to variable names of group names on which to group spelling replacements, i.e.
                i.e. group is pc01_state_name if fix_spelling pc01_district_name                       */
  /* Example syntax:
  
  fix_spelling district_name_fm, targetfield(pc01_district_name) group(state_name_fm) targetgroup(pc01_state_name) ///
    src($keys/pc01_district_key) gen(new_district_name)
  
  where target is universal naming syntax in keys, and state_name_fm / district_name_fm are names in set */
  /*******************************************************************************************************/
  cap prog drop fix_spelling
  prog def fix_spelling
  {
    syntax varname(min=1 max=1), SRCfile(string) [GEN(name) GROUP(varlist) TARGETfield(string) TARGETGROUP(string) keepall replace FUZZINESS(real 2)]
  
    /* put variable fix spelling in `varname' because we need arg "`1'" open */
    tokenize `varlist'
    local varname `1'
    
    /* need to specify either generate or replace, not both */
    if (mi("`gen'") & mi("`replace'")) | (!mi("`gen'") & !mi("`replace'")) {
      display as error "fix_spelling: Need to specify either generate or replace, not both"
      exit 1
    }
  
    /* if replace is set, create a temp var to be generated */
    if !mi("`replace'") {
      tempvar gen
    }
    
    /* if group is empty, need to create a master group that is the entire file */
    if mi("`group'") {
      tempvar __GROUP
      gen `__GROUP' = 1
      local nogroup = 1
      local group "`__GROUP'"
    }
    
    /* for now, assume we have a source file */
  
    /* create the master list */
    qui {
      preserve
  
      /* open the synonym list: master location key */
      use "`srcfile'", clear
  
      /* if TARGETFIELD was specified, rename the target to the variable we want to match */
      if !mi("`targetfield'") {
        ren `targetfield' `varname'
      }
  
      /* if TARGETGROUP was specified, rename group variables to match key used to spellcheck */
      if !mi("`targetgroup'") {
        
         /* loop through each element in group and targetgroup to rename each variable from targetgroup as named in master set */
         /* group -> tokens `1' -> `n', while we loop over target group. Need to do this because can't tokenize two strings */
         tokenize `group'
         local i = 1
         foreach targetgroup_var in `targetgroup' {
           cap confirm variable `targetgroup_var'
           if _rc {
             disp as error "ERROR fix_spelling: key file missing targetgroup var `targetgroup_var'"
             exit 123
           }
           ren `targetgroup_var' ``i'' 
           local i = `i' + 1
         }         
       }
      
      /* define a group if none exists */
      if !mi("`nogroup'") gen `__GROUP' = 1
      
      /* save clean, unique synonym list as stata file */
      tempfile SPELLING_MASTER_LIST
      keep `group' `varname'
      duplicates drop
      sort `group' `varname'
      save `SPELLING_MASTER_LIST', replace
      restore
      
      /* create a list of unmatched names */
      preserve
    
      keep `varname' `group'
      duplicates drop
    
      /* get rid of exact matches - these will work well */
      merge 1:1 `group' `varname' using `SPELLING_MASTER_LIST', gen(_merge1)
      keep if _merge1 == 1
    
      /* if nothing left, then the original list is fine and we're done */
      qui count
      if r(N) == 0 {
        restore
        noi di "100% of names matched exactly. No fuzzy matching necessary"
        
        /* pass variable back into original or specified gen variable */
        gen `gen' = `varname'
        
        /* drop group var from main set if no group was specified */
        if !mi("`nogroup'") drop `__GROUP'
        exit
      }
  
      /* set tempfile to outfile spelling errors */
      tempfile spelling_errors
  
      /* otherwise, go to the fuzzy merge */
      lev_merge `group' using `SPELLING_MASTER_LIST', s1(`varname') outfile(`spelling_errors') fuzziness(`fuzziness')
  
      /* set tempfile for spelling corrections */
      tempfile SPELLING_CORRECTIONS
      
      /* review lev merge results */
      use `spelling_errors', clear
    
      /* exit if no matches */
      count
      if `r(N)' == 0 exit
    
      /* keep best match for everything in badly-spelled set */
      keep if keep_master == 1
      keep `group' `varname'_master `varname'_using lev_dist
    
      /* fix names and merge back to the original dataset */
      ren `varname'_master `varname'
      ren `varname'_using `gen'
      ren lev_dist `gen'_dist
      save `SPELLING_CORRECTIONS', replace
      restore
    
      /* tag exact matches (this merge only adds _merge_exact) */
      merge m:1 `group' `varname' using `SPELLING_MASTER_LIST', gen(_merge_exact)
      drop if _merge_exact == 2
      
      /* then get fuzzy matches */
      merge m:1 `group' `varname' using `SPELLING_CORRECTIONS', gen(_merge_fuzzy)
      assert _merge_fuzzy != 2
    
      /* if we have an exact match, shouldn't have a fuzzy match */
      assert _merge_fuzzy == 1 if _merge_exact == 3
      
      /* add exact matches */
      replace `gen' = `varname' if _merge_exact == 3
      replace `gen'_dist = 0 if _merge_exact == 3
      drop _merge_exact _merge_fuzzy
    
      /* if keepall specified, get places that didn't match */
      if !mi("`keepall'") {
    
        /* merge the spell-checked data back to the master list within the group */
        ren `varname' `varname'_SP
        ren `gen' `varname'
        merge m:1 `group' `varname' using `SPELLING_MASTER_LIST', nogen keepusing(`varname')
        ren `varname' `gen'
        ren `varname'_SP `varname'
      }
  
      /* if group was not specified (or place is state so there is no group), drop group var */
      if !mi("`nogroup'") drop `__GROUP'
    }
    /* if replace was specified */
    if !mi("`replace'") {
  
      /* show replacements made */
      tempvar tag
      qui egen `tag' = tag(`varname') if !mi(`gen') & `gen' != `varname'
      disp_nice "Spelling fixes and Masala-Levenshtein distances:"
      list `varname' `gen' `gen'_dist if `tag'
      
      /* replace original var, show what was done, and drop the distance */
      qui replace `varname' = `gen' if !mi(`gen')
      drop `gen' `gen'_dist
    }
  
  }
  end
  /* *********** END program fix_spelling ***************************************** */
  
  /**********************************************************************************/
  /* program lev_review : Reviews lev_merge results and calls lev_process  */
  /***********************************************************************************/
  cap prog drop lev_review
  prog def lev_review
  {
    syntax varlist, s1(string) master(string) using(string) [keepusing(passthru)]
  
    /* ensure a masala merge output file is open */
    cap confirm var keep_master
    if _rc {
      di "You must open the lev_merge output file before running this program."
    }
    
    /* count and report matches that are exact, but with alternatives */
    /* these are places where keep_master == 0 & lev_dist == 0 */
    qui bys `s1'_master: egen _min_dist = min(lev_dist)
    qui bys `s1'_master: egen _max_keep = max(keep_master)
  
    qui count if _max_keep == 0 & _min_dist == 0
    if `r(N)' > 0 {
      di "+-------------------------------------" _n "| These are exact matches, where alternate good matches exist." _n ///
        "| keep_master is 0, but lev_process() will keep the observations with lev_dist == 0." _n ///
          "+-------------------------------------" 
      list `varlist' `s1'* lev_dist if _max_keep == 0 & _min_dist == 0
    }
    qui drop _max_keep _min_dist
  
    /* visually review places with high lev_dist that script kept -- they look good. */
    qui count if keep_master == 1 & lev_dist > 1
    if `r(N)' > 1 {
      disp_nice "These are high cost matches, with no good alternatives. keep_master is 1."
      list `varlist' `s1'* lev_dist if keep_master == 1 & lev_dist > 1
    }
  
    /* run lev_process, and then show the unmatched places */
    lev_process `varlist', s1(`s1') master(`master') using(`using') `keepusing'
  
    /* tag each name so it doesn't appear more than once */
    qui egen _ntag = tag(`varlist' `s1')
  
    /* list unmatched places in a nice order */
    qui gen _matched = _masala_merge == 3
    gsort _matched -_ntag `varlist' `s1'
  
    /* ensure we don't trigger obs. nos. out of range in final list, by counting observations */
    qui count
    if `r(N)' < 200 {
      local limit `r(N)'
    }
    else {
      local limit 200
    }
  
    /* list unmatched places */
    qui count if _masala_merge < 3 & _ntag in 1/`limit'
    if `r(N)' {
      disp_nice "This is a sorted list of some places that did not match. Review for ideas on how to improve"
      list `varlist' `s1' _masala_merge if _masala_merge < 3 & _ntag in 1/`limit', sepby(`varlist')
    }
  
    drop _ntag _matched
  }
  end
  /* *********** END program lev_review ***************************************** */
  
  /**********************************************************************************/
  /* program lev_process : Rejoins the initial files in a lev_merge           */
  /**********************************************************************************/
  cap prog drop lev_process
  prog def lev_process
  {
    syntax varlist, s1(string) master(string) using(string) [keepusing(passthru)]
  
    qui {
      /* override keep_master if lev_dist is zero. */
      replace keep_master = 1 if lev_dist == 0
      
      /* keep highly reliable matches only */
      keep if keep_master == 1
    
      /* drop all lev merge's variables (keep lev_dist) */
      keep `varlist' `s1'* lev_dist
  
      /* bring back master dataset */
      gen `s1' = `s1'_master
      merge 1:m `varlist' `s1' using `master', gen(_masala_master)
  
      /* fill in master fuzzy-string from unmatched data on master side */
      replace `s1'_master = `s1' if mi(`s1'_master)
      drop `s1'
      
      /* bring back using dataset */
      gen `s1' = `s1'_using
    
      merge m:1 `varlist' `s1' using `using', `keepusing' gen(_masala_using)
  
      /* fill in using fuzzy-string from unmatched data on using side  */
      replace `s1'_using = `s1' if mi(`s1'_using)
      drop `s1'
      
      /* set `s1' to the master value */
      ren `s1'_master `s1'
      
      /* fill in using values when _m == 2 */
      replace `s1' = `s1'_using if mi(`s1')
    }
  
    /* Assertion: if we couldn't match back to the using, it must be unmatched from the master side */
    assert _masala_master == 2 if _masala_using == 1
  
    /* show merge result */
    disp_nice "Results of lev_merge (counting unique strings only): "
    
    /* tag each name so it doesn't appear more than once */
    qui egen ntag = tag(`varlist' `s1')
  
    /* create a standard merge output variable */
    qui gen _masala_merge = 1 if _masala_master == 2
    qui replace _masala_merge = 2 if _masala_using == 2
    qui replace _masala_merge = 3 if _masala_using == 3 & _masala_master == 3
    drop _masala_master _masala_using
    label values _masala_merge _merge
  
    /* show results */
    table _masala_merge if ntag
    qui drop ntag
  }
  end
  /* *********** END program lev_process ***************************************** */
  
  /**********************************************************************************/
  /* program review_merge : call right after a merge to review potential matches    */
  /***********************************************************************************/
  cap prog drop review_merge
  prog def review_merge
  {
    syntax varlist [if/], [merge(string) list(varlist) SHOWall]
  
    tempvar tag
    
    if mi("`merge'") {
      local merge _merge
    }
  
    /* `showall' parameter determines whether we limit to _merge < 3 or show all results */
    if !mi("`showall'") {
      local show 1
    }
    else {
      local show `merge' < 3
    }
  
    /* need something in `if' if nothing passed in */
    if mi("`if'") {
      local if 1
    }
    
    /* separate list with sepby() if more than one variable in varlist */
    tokenize "`varlist'"
    if !mi("`2'") {
      local sepby sepby(`1')
    }
  
    /* only show each posisble match once */
    egen `tag' = tag(`varlist')
    
    sort `varlist' `list'
    list `varlist' `list' `merge' if `show' & `if' & `tag', `sepby'
    drop `tag'
  }
  end
  /* *********** END program review_merge ***************************************** */
  
  /***********************************************************************************/
  /* program create_merge_fragments : Call right after a merge to create separate
                                      files of the unmatched pieces                  */
  /***********************************************************************************/
  cap prog drop create_merge_fragments
  prog def create_merge_fragments
  {
  
    /* idea is we had:
   file1: merge_vars a b c
   file2: merge_vars d e f
  
  we want to create file1 and file2 leftovers. hard part is getting the variables right.
  
  syntax option:
  - call with the completed merge file open, pass original master() and using() files back in.
  
    */
    syntax anything, master(string) using(string) [merge(string) suffix(string)]
  
    /* set default values for merge and suffix locals */
    if mi("`merge'") local merge _merge
    if mi("`suffix'") local suffix unmatched
    
    /* hack work to get m:1, 1:1, or 1:m */
    local merge_type = substr("`anything'", 1, 3)
  
    if !inlist("`merge_type'", "1:1", "m:1", "1:m") {
      di "Must specify 1:1, m:1 or 1:m as in merge syntax"
      error 123
    }
    local master_type = substr("`merge_type'", 1, 1)
    local using_type = substr("`merge_type'", 3, 1)
    
    local varlist = substr("`anything'", 4, .)
    
    /* confirm varlist is a varlist */
    confirm var `varlist'
  
    /* we want to leave this unaltered, so wrap everything in a preserve / restore */
    preserve
  
    /* keep only the matches and drop _merge */
    keep if `merge' == 3
    drop `merge'
    
    /* save the file with the matches. all we need is the varlist */
    keep `varlist'
  
    /* we only need one copy of each match, this allows everything below to be m:1 */
    duplicates drop
    tempfile merge3
    save `merge3', replace
    
    /* create master only file */
    use `master', clear
  
    /* merge it to list of matches */
    /* 1:m is con->village. merged file will have many repeated cons */
    /* m:1 is village->con. merged file will have each village once */
    /* 1:1 obviously has each side once */
    merge 1:`using_type' `varlist' using `merge3'
  
    /* now we want to keep only the non-matches */
    keep if `merge' == 1
  
    /* there should not be any using, if we just ran this merge */
    assert `merge' != 2
  
    /* drop _merge and save fragment file */
    drop `merge'
    save `master'_`suffix', replace
  
    /* repeat process for using side */
    use `using', clear
  
    /* merge it to list of matches */
    /* 1:m is con->village. merged file will have each village once */
    /* m:1 is village->con. merged file will have cons repeated */
    /* 1:1 obviously has each side once */
    merge `master_type':1 `varlist' using `merge3'
  
    /* now we want to keep only the non-matches */
    keep if `merge' == 1
  
    /* there should not be any using, if we just ran this merge */
    assert `merge' != 2
  
    /* drop _merge and save fragment file */
    drop `merge'
    save `using'_`suffix', replace
    
    restore
  
    /* report what happened */
    di "Created files with merge fragments:"
    di "  master: `master'_`suffix'"
    di "  using: `using'_`suffix'"
  }
  end
  /* *********** END program create_merge_fragments ***************************************** */
  
  /*****************************************************************************************************/
  /* program synonym_fix : Replace strings for names that have multiple variants but one main version, */
  /*                       using a externally-supplied dictionary                                      */
  /*                       i.e. uttaranchal -> uttarakhand                                             */
    /* external file has the following columns:
        "master"             : the target name
        "pc01_district_name" : variable that we are replacing
        "`group'"            : a list of variables that define the context in which to match the name
                               named according to standard pc`year'_`place'_name
     external file must be in CSV format.
  */
  /* options targetfield and targetgroup allow variables in dataset to differ from variables in .csv
  targetfield:  refers to column name in external csv file named according to pc`year'_`place'_name
  targetgroup:  refers to the columns after the main name column on which to group replacements, i.e.
                synonym_fix pc01_district_name has group pc01_state_name
  */
  
  /*****************************************************************************************************/
  cap prog drop synonym_fix
  prog def synonym_fix

    syntax varname, SYNfile(string) [GENerate(name) replace group(varlist) TARGETfield(string) TARGETGROUP(string)] 
    
    /* put variable to replace using synonym_fix in `varname' */
    tokenize `varlist'
    local varname `1'
    
    /* require generate or replace */
    if (mi("`generate'") + mi("`replace'")) != 1 {
      display as error "synonym_fix: generate or replace must be specified"
      exit 1
    }
    
    /* if no generate specified, make replacements to passed in variable */
    if mi("`generate'") {
      local name = "`varname'"
    }
  
    /* if generate specified, copy the variable into the new slot and then change it gradually */
    else {
      gen `generate' = `varname'
      local name = "`generate'"
    }
    
    qui {
      
      /* verify 'master' variable is not in use in order to make replacements */
      /* master refers to first column in csv that contains the correct string replacement value */
        cap confirm variable master
        if !_rc {
          display as error "'master' variable already exist. synonym_fix() needs this varname to be free."
          exit 123
        }
        
        /* insheet external csv from ~/iecmerge/pc[]/place_synonyms/  */
        preserve
  
        /* read the synfile by using import delimited with ufs-8 encoding */
        import delimited "`synfile'", clear delim(",") varn(1) encoding("utf-8")
        tostring `varname', replace force
        recast str `varname'
        /* targetfield: renaming the target name variable from the replacement file to match the master dataset */
  
        /* if a target variable field was specified, rename the insheeted target field variable to match the varname in master dataset */
        if !mi("`targetfield'") {
          cap confirm variable `targetfield'
          if _rc {
            disp as error "ERROR synonym_fix: input file missing target var `targetfield'"
            exit 123
          }
          ren `targetfield' `varname'
        }
  
        /* otherwise, if these names are the same, do a clean check to confirm synfix has the right field */
        else {
          cap confirm variable `varname'
          if _rc {
            disp as error "ERROR synonym_fix: input file missing variable `varname'"
            exit 123
          }
        }
        
        /* We require all replacements to be strings. Enforce this in case Stata
           imported an encoded list as numbers. */
        tostring master `varname', replace force
      
       /* targetgroup: renaming group variables from replacement file to match master dataset */
  
       /* if a target group was specified, rename the target group names from csv to match master group varnames passed in with group() */
       /* if targetgroup is specified it is implicit that group has been specified */
        if !mi("`targetgroup'") {
          
          /* loop through each element in group and targetgroup to rename each variable from targetgroup as named in master set */
         /* group -> tokens `1' -> `n', while we loop over target group. Need to do this because can't tokenize two strings */
          tokenize `group'
          local i = 1
          foreach targetgroup_var in `targetgroup' {
            cap confirm variable `targetgroup_var'
            if _rc {
              disp as error "ERROR synonym_fix: input file missing targetgroup var `targetgroup_var'"
              exit 123
            }
            ren `targetgroup_var' ``i'' 
            local i = `i' + 1
          }         
        }
  
        /* assert no duplicates on replacement value */
        cap bys `varname' `group': assert _N == 1
        if _rc {
          display as error ""
          display as error "ERROR synoynm_fix: Synonym file must be unique on value to be replaced (column 2) and optional merge variables."
          noi di "Target group was: `group'"
          noi duplicates list `varname' `group'
          exit 123
        }
        
        /* write the dictionary of replacements */
        tempfile dict
      
        /* the csv should already be formatted lower and trimmed but perform replace in case it's not */
        replace master = trim(lower(master))
        foreach item in `group' {
          replace `item' = trim(lower(`item'))
        }
      
        /* drop empty rows in case stata insheeted blank rows from the csv */      
        drop if mi(master)
        save "`dict'", replace
        restore
        
        /* prepare group vars for merge */
        if !mi("`group'") {
          foreach v of varlist(`group') {
            tostring `v', replace force
            replace `v' = trim(lower(`v'))
          }
        }
        
        /* merge passed in variable to the synonym column */
        merge m:1 `group' `varname' using `dict', keepusing(master)
        drop if _merge == 2
      }    
  
      /* make the actual string replacement - put out of quietly block so user can see what happened */
      replace `name' = master if !mi(master)
  
      drop master _merge

  end
  /* *********** END program synonym_fix ***************************************** */
  
  /**********************************************************************************/
  /* program str_fix : Manual spelling corrections from a master list              */
  /*                       i.e. uttaranchal -> uttarakhand                         */
  /*   external CSV file has the following columns:
        "goodstring"                        : the field with the corrected entries   
        keys (e.g. pc01_state_id ac07_id): the fields that uniquely identify a row in this file */
  
  // str_fix ac07_name using $tmp/fixes.csv, replace keys(pc01_state_id ac07_id)
  
  /*****************************************************************************************************/
  cap prog drop str_fix
  prog def str_fix
  {
    syntax varname using, [GENerate(name) replace KEYs(varlist)] 
    
    /* put variable to replace using str_fix in `varname' */
    tokenize `varlist'
    local varname `1'
    
    /* require generate or replace */
    if (mi("`generate'") + mi("`replace'")) != 1 {
      display as error "str_fix: generate or replace must be specified"
      exit 1
    }
    
    /* if no generate specified, make replacements to passed in variable */
    if mi("`generate'") {
      local name = "`varname'"
    }
  
    /* if generate specified, copy the variable into the new slot and then change it gradually */
    else {
      gen `generate' = `varname'
      local name = "`generate'"
    }
    
    qui {
      
      /* verify '__replacements' variable is not in use in order to make replacements */
      /* this will hold the correct string replacement value */
      cap confirm variable __replacements
      if !_rc {
        display as error "'__replacements' variable already exist. str_fix() needs this varname to be free."
        exit 543
      }
      
      /* insheet external csv string list */
      preserve
      
      /* read the synfile by using import delimited with utf-8 encoding */
      import delimited `using', clear delim(",") varn(1) encoding("utf-8")
      
      /* confirm this file has the vars we need it to */
      confirm variable goodstring `keys'
      if _rc {
        disp as error "ERROR str_fix: input file should have variables goodstring, `keys'"
        exit 543
      }
      
      /* assert no duplicates on replacement value */
      cap bys `keys': assert _N == 1
      if _rc {
        display as error ""
        display as error "ERROR str_fix: string file must be unique on `keys'."
        noi duplicates list `keys'
        exit 543
      }
      
      /* write the string replacements to a stata file */
      tempfile dict
    
      /* drop empty rows in case stata insheeted blank rows from the csv */      
      drop if mi(goodstring)
  
      /* rename "goodstring" to __replacements to avoid accidental conflicts with original file */
      ren goodstring __replacements
      save "`dict'", replace
  
      /* go back to the original dataset */
      restore
      
      /* merge in the replacement values on the specified keys */
      merge m:1 `keys' using `dict', keepusing(__replacements) nogen keep(master match)
      
      /* force to a string just in case the file contained numbers as strings and Stata defaulted to int */
      tostring __replacements, replace
    }    
  
    /* make the actual string replacement - put out of quietly block so user can see what happened */
    replace `name' = __replacements if !mi(__replacements)
  
    drop __replacements
  }
  end
  /* *********** END program str_fix ***************************************** */
  
}
